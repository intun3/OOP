## ЛР3

Кросс-валидация - это техника оценки производительности модели машинного обучения, которая позволяет использовать все имеющиеся данные для обучения и оценки модели. Основная идея заключается в разбиении данных на несколько частей, где одна часть используется для оценки модели, а остальные - для обучения.

Основные этапы кросс-валидации:

1. Разделение данных на K равных частей (обычно K = 5 или K = 10).
2. Тренировка модели K раз, используя K-1 частей как обучающую выборку и оставшуюся часть как тестовую.
3. Вычисление средней производительности модели по всем K итерациям.

Преимущества кросс-валидации:

- Использование всех имеющихся данных для обучения и оценки модели.
- Получение более надежной оценки производительности модели.
- Снижение риска переобучения.
- Возможность выявления зависимости производительности модели от состава обучающей выборки.

Кросс-валидация широко применяется в машинном обучении для оценки модели, выбора гиперпараметров и сравнения различных моделей.****

## ЛР4
DecisionTreeClassifier - это один из алгоритмов машинного обучения, используемый для задач классификации. Он представляет собой решающее дерево - древовидную структуру, где каждый внутренний узел соответствует проверке некоторого признака объекта, ветви исходящие из узла соответствуют возможным значениям этого признака, а листовые узлы соответствуют решениям о классификации объекта.

Основные характеристики DecisionTreeClassifier:

1. Построение модели происходит путем последовательного разбиения данных на подмножества на основе наиболее информативных признаков.
2. Алгоритм пытается найти такое разбиение данных, которое максимально увеличивает "чистоту" подмножеств, то есть стремится к тому, чтобы в каждом подмножестве были объекты только одного класса.
3. Важными параметрами модели являются: критерий разбиения (например, коэффициент Джини или информационный критерий), максимальная глубина дерева, минимальное количество объектов в листовом узле и другие.
4. Преимущества: простота интерпретации, возможность работы с разными типами данных, устойчивость к выбросам и шуму. Недостатки: склонность к переобучению, зависимость от исходных данных.

DecisionTreeClassifier является одним из базовых и широко используемых алгоритмов в машинном обучении, особенно в задачах классификации.

## ЛР5
Логистическая регрессия - это статистический метод, используемый для решения задач классификации. Он позволяет предсказывать вероятность принадлежности объекта к одному из двух (или нескольких) классов на основе значений одного или нескольких предикторов (независимых переменных).

Основные особенности логистической регрессии:

1. Логистическая регрессия использует логистическую функцию для моделирования вероятности бинарного исхода. Логистическая функция обеспечивает ограничение вероятности в диапазоне от 0 до 1.
![[Pasted image 20240522103256.png]]

где \(\mathbf{w}\) - вектор весовых коэффициентов, \(\mathbf{x}\) - вектор признаков объекта.

2. Модель подбирает параметры \(\mathbf{w}\) таким образом, чтобы максимизировать правдоподобие данных.

3. Логистическая регрессия хорошо работает, когда зависимая переменная является бинарной (0 или 1), но также может быть обобщена на многоклассовые задачи.

4. Преимущества: простота интерпретации, возможность оценки вероятностей, устойчивость к мультиколлинеарности. Недостатки: чувствительность к выбросам, предположение о линейной зависимости логарифмического шанса от признаков.

Логистическая регрессия широко применяется в задачах классификации в медицине, маркетинге, финансах и многих других областях.
## ЛР6
Линейная регрессия - это один из основных методов машинного обучения, используемый для решения задач прогнозирования и моделирования зависимостей между переменными.

Ключевые особенности линейной регрессии:

1. Модель представляет собой линейную функцию от одной или нескольких независимых переменных (признаков):

![[Pasted image 20240522103352.png]]
где \(\mathbf{x}\) - вектор признаков, \(\mathbf{w}\) - вектор весовых коэффициентов, \(b\) - свободный член.

2. Цель - найти такие значения коэффициентов \(\mathbf{w}\) и \(b\), которые минимизируют ошибку между предсказанными и фактическими значениями зависимой переменной \(y\).

3. Существует несколько методов оценки коэффициентов, например, метод наименьших квадратов.

4. Линейная регрессия хорошо работает, когда зависимость между переменными линейна, но может быть обобщена на нелинейные случаи с помощью преобразования признаков.

5. Преимущества: простота интерпретации, устойчивость к шуму, возможность оценки значимости признаков. Недостатки: чувствительность к выбросам, предположение о линейной зависимости.

Линейная регрессия широко применяется в различных областях, таких как экономика, финансы, социология, инженерия и многих других, для моделирования и прогнозирования количественных переменных.
## ЛР8
Пайплайн для одномерной линейной регрессии - это последовательность шагов, которая позволяет построить и оценить модель линейной регрессии для прогнозирования одной зависимой переменной на основе одного признака.

Основные шаги в таком пайплайне:

1. Загрузка и предварительная обработка данных:
   - Импортировать необходимые библиотеки (например, Numpy, Pandas, Scikit-learn)
   - Загрузить данные (например, из CSV-файла)
   - Проверить и при необходимости обработать пропущенные значения

2. Разделение данных на обучающую и тестовую выборки:
   - Использовать функцию train_test_split() из Scikit-learn
   - Указать размер тестовой выборки (например, 20% от общего объема данных)

3. Создание и обучение модели линейной регрессии:
   - Импортировать класс LinearRegression() из Scikit-learn
   - Создать экземпляр модели и вызвать метод fit() для обучения на обучающей выборке

4. Оценка качества модели:
   - Использовать метрики качества, такие как R-квадрат, среднеквадратическая ошибка (MSE) или среднее абсолютное отклонение (MAE)
   - Вычислить эти метрики на тестовой выборке с помощью соответствующих функций

5. Визуализация результатов:
   - Построить график фактических и предсказанных значений
   - Визуализировать распределение остатков (разница между фактическими и предсказанными значениями)

6. Использование модели для прогнозирования:
   - Использовать метод predict() модели для получения прогнозов на новых данных

Такой пайплайн можно реализовать с помощью библиотек Pandas и Scikit-learn на языке Python. Он обеспечивает структурированный подход к построению и оценке модели линейной регрессии.
## ЛР9
Пайплайн для многомерной линейной регрессии следует похожим шагам, но с некоторыми дополнительными особенностями, связанными с работой с несколькими признаками. Вот основные шаги:

1. Загрузка и предварительная обработка данных:
   - Импортировать необходимые библиотеки (Numpy, Pandas, Scikit-learn)
   - Загрузить данные (например, из CSV-файла)
   - Обработать пропущенные значения (заполнить средними, медианами или другими подходящими методами)

2. Разделение данных на обучающую и тестовую выборки:
   - Использовать функцию train_test_split() из Scikit-learn
   - Указать размер тестовой выборки (например, 20% от общего объема данных)

3. Создание и обучение модели многомерной линейной регрессии:
   - Импортировать класс LinearRegression() из Scikit-learn
   - Создать экземпляр модели и вызвать метод fit() для обучения на обучающей выборке, передав все признаки

4. Оценка качества модели:
   - Вычислить коэффициент детерминации R-квадрат на тестовой выборке
   - Рассчитать среднеквадратическую ошибку (MSE) или среднее абсолютное отклонение (MAE)
   - Проанализировать значимость коэффициентов регрессии (р-значения)

5. Визуализация результатов:
   - Построить график фактических и предсказанных значений
   - Визуализировать распределение остатков (разница между фактическими и предсказанными значениями)
   - Построить графики зависимости зависимой переменной от каждого признака

6. Использование модели для прогнозирования:
   - Использовать метод predict() модели для получения прогнозов на новых данных, передав в него матрицу признаков

Дополнительные шаги:
- Отбор признаков: использовать методы, такие как пошаговая регрессия, для выбора наиболее значимых признаков
- Проверка предположений: проверить нормальность распределения остатков, гомоскедастичность, отсутствие мультиколлинеарности
- Регуляризация: применить методы регуляризации, такие как Ridge или Lasso, для борьбы с переобучением

Этот пайплайн позволяет построить и оценить модель многомерной линейной регрессии, используя библиотеки Pandas и Scikit-learn в Python.
## ЛР10
Полиномиальная регрессия - это расширение линейной регрессии, которое позволяет моделировать нелинейные зависимости между зависимой переменной и независимыми переменными.

Основные шаги пайплайна для полиномиальной регрессии:

1. Загрузка и предварительная обработка данных:
   - Импортировать необходимые библиотеки (Numpy, Pandas, Scikit-learn)
   - Загрузить данные (например, из CSV-файла)
   - Обработать пропущенные значения при необходимости

2. Разделение данных на обучающую и тестовую выборки:
   - Использовать функцию train_test_split() из Scikit-learn
   - Указать размер тестовой выборки (например, 20% от общего объема данных)

3. Преобразование признаков в полиномиальные:
   - Использовать класс PolynomialFeatures() из Scikit-learn
   - Задать степень полинома (например, 2 для квадратичной зависимости)
   - Преобразовать исходные признаки в полиномиальные

4. Создание и обучение модели полиномиальной регрессии:
   - Импортировать класс LinearRegression() из Scikit-learn
   - Создать экземпляр модели и вызвать метод fit() для обучения на обучающей выборке, передав преобразованные признаки

5. Оценка качества модели:
   - Вычислить коэффициент детерминации R-квадрат на тестовой выборке
   - Рассчитать среднеквадратическую ошибку (MSE) или среднее абсолютное отклонение (MAE)

6. Визуализация результатов:
   - Построить график фактических и предсказанных значений
   - Визуализировать распределение остатков (разница между фактическими и предсказанными значениями)
   - Построить графики зависимости зависимой переменной от каждого признака

7. Использование модели для прогнозирования:
   - Преобразовать новые данные в полиномиальные признаки
   - Использовать метод predict() модели для получения прогнозов

Полиномиальная регрессия позволяет моделировать нелинейные зависимости, что может значительно улучшить качество прогнозов по сравнению с простой линейной регрессией. Однако важно избегать переобучения, поэтому часто используются методы регуляризации, такие как Ridge или Lasso.